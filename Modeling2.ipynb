{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Modeling2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y-7DZUgoabB2",
        "W3xw5Z2fabB5",
        "fKdQbd1BabB8"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunyongPark/Recommendation/blob/master/Modeling2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTzjmT5qhdQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "dec58f43-843c-472d-fa26-79a81561b415"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzIUc1aIhe3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "9cfec56d-44a0-492e-94ee-134f2f96b80f"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUSnBlhBhfv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "outputId": "d46f7b3f-d424-4cc4-898b-3e0f9751a463"
      },
      "source": [
        "!pip install tensorflow==1.7"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/4a/42ba8d00a50a9fafc88dd5935246ecc64ffe1f6a0258ef535ffb9652140b/tensorflow-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (48.0MB)\n",
            "\u001b[K     |████████████████████████████████| 48.0MB 93kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.31.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.35.1)\n",
            "Collecting tensorboard<1.8.0,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.7) (49.6.0)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (3.2.2)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (3.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=19b5dbfd44455e0ebc6db039ca03464ab3514b6e5bb30358a9412f3c02f6dab1\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWkPYpfCiTrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "0d70afa5-fc03-4eca-9317-c30af8b4224a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHv3nE-hBa5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cmq4lNFabBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "273b6351-188d-44db-f9ae-bb396f8327a0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from six import next\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import coo_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARU3BzHYabBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data100k():\n",
        "    global PERC\n",
        "    df = read_process(\"/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.data\", sep=\"\\t\")\n",
        "    rows = len(df)\n",
        "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
        "    split_index = int(rows * PERC)\n",
        "    df_train = df[0:split_index]\n",
        "    df_test = df[split_index:].reset_index(drop=True)\n",
        "    return df_train, df_test\n",
        "\n",
        "def get_UserData100k():\n",
        "    col_names = [\"user\", \"age\", \"gender\", \"occupation\",\"PostCode\"]\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.user', sep='|', header=None, names=col_names, engine='python')\n",
        "    del df[\"PostCode\"]\n",
        "    df[\"user\"]-=1\n",
        "    df=pd.get_dummies(df,columns=[ \"age\", \"gender\", \"occupation\"])\n",
        "    del df[\"user\"]\n",
        "    return df.values\n",
        "\n",
        "def get_ItemData100k():\n",
        "    col_names = [\"movieid\", \"movietitle\", \"releasedate\", \"videoreleasedate\",\"IMDbURL\"\n",
        "                ,\"unknown\",\"Action\",\"Adventure\",\"Animation\",\"Childrens\",\"Comedy\",\"Crime\",\"Documentary\"\n",
        "                ,\"Drama\",\"Fantasy\",\"FilmNoir\",\"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"SciFi\",\"Thriller\"\n",
        "                ,\"War\",\"Western\"]\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.item', sep='|', header=None, names=col_names, engine='python')\n",
        "    df['releasedate'] = pd.to_datetime(df['releasedate'])\n",
        "    df['year'],df['month']=zip(*df['releasedate'].map(lambda x: [x.year,x.month]))\n",
        "    df['year']-=df['year'].min()\n",
        "    df['year']/=df['year'].max()\n",
        "    df['year']=df['year'].fillna(0.0)\n",
        "\n",
        "    del df[\"month\"]\n",
        "    del df[\"movietitle\"]\n",
        "    del df[\"releasedate\"]\n",
        "    del df[\"videoreleasedate\"]\n",
        "    del df[\"IMDbURL\"]\n",
        "  \n",
        "    df[\"movieid\"]-=1\n",
        "    del  df[\"movieid\"]\n",
        "    return df.values \n",
        "\n",
        "def read_process(filname, sep=\"\\t\"):\n",
        "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
        "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
        "    df[\"user\"] -= 1\n",
        "    df[\"item\"] -= 1\n",
        "    for col in (\"user\", \"item\"):\n",
        "        df[col] = df[col].astype(np.int32)\n",
        "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_data100k():\n",
        "    global PERC\n",
        "    df = read_process(\"/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.data\", sep=\"\\t\")\n",
        "    rows = len(df)\n",
        "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
        "    split_index = int(rows * PERC)\n",
        "    df_train = df[0:split_index]\n",
        "    df_test = df[split_index:].reset_index(drop=True)\n",
        "    return df_train, df_test\n",
        "\n",
        "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
        "    with tf.device(DEVICE):\n",
        "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
        "        assert global_step is not None\n",
        "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
        "        cost = tf.add(cost_l2, regularizer)\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
        "    return cost, train_op\n",
        "\n",
        "def optimizationSparse(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
        "    global_step = tf.train.get_global_step()\n",
        "    assert global_step is not None\n",
        "    cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
        "    cost = tf.add(cost_l2, regularizer)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
        "    return cost, train_op\n",
        "\n",
        "def clip(x):\n",
        "    return np.clip(x, 1.0, 5.0) #벗어나는 값들 위치시키기\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc1TOwrPabB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShuffleIterator(object):\n",
        "\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        self.inputs = inputs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_cols = len(self.inputs)\n",
        "        self.len = len(self.inputs[0])\n",
        "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "\n",
        "    def next(self):\n",
        "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
        "        out = self.inputs[ids, :] #뭐임?\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "\n",
        "\n",
        "class OneEpochIterator(ShuffleIterator):\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
        "        if batch_size > 0:\n",
        "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
        "        else:\n",
        "            self.idx_group = [np.arange(self.len)]\n",
        "        self.group_id = 0\n",
        "\n",
        "    def next(self):\n",
        "        if self.group_id >= len(self.idx_group):\n",
        "            self.group_id = 0\n",
        "            raise StopIteration\n",
        "        out = self.inputs[self.idx_group[self.group_id], :]\n",
        "        self.group_id += 1\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "    \n",
        "\n",
        "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
        "    with tf.device(DEVICE): \n",
        "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\")\n",
        "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\")\n",
        "        \n",
        "        \n",
        "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
        "\n",
        "\n",
        "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\")\n",
        "\n",
        "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\")\n",
        "\n",
        "    return infer, regularizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-7DZUgoabB2",
        "colab_type": "text"
      },
      "source": [
        "## Embedding + dot.product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PxXTuyAabB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
        "    with tf.device(DEVICE): \n",
        "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
        "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
        "        \n",
        "        \n",
        "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(ul1mf.shape)\n",
        "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(il1mf.shape)\n",
        "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
        "        print(InferInputMF.shape)\n",
        "\n",
        "\n",
        "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
        "\n",
        "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
        "    return infer, regularizer, ul1mf, il1mf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s4dYfppufGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction_matrix(user_dict, item_dict):\n",
        "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
        "  for i in range(len(user_dict)):\n",
        "    for j in range(len(item_dict)):\n",
        "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C1X4jXOabB5",
        "colab_type": "text"
      },
      "source": [
        "- InferInputMF의 shape[1] 이 의미하는 것 (100) \n",
        "- mf 를 왜쓰냐\n",
        "- mf 의 output 이 어떠한 방식으로 prediction의 형태를 띄느냐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xw5Z2fabB5",
        "colab_type": "text"
      },
      "source": [
        "## GraphRec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKdQbd1BabB8",
        "colab_type": "text"
      },
      "source": [
        "## 뜯어보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9o4MLwHabCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def GraphRec():\n",
        "    BATCH_SIZE = 1000\n",
        "    USER_NUM = 943\n",
        "    ITEM_NUM = 1682\n",
        "\n",
        "    #With Graph Features\n",
        "    MFSIZE=50\n",
        "    UW=0.05\n",
        "    IW=0.02\n",
        "    LR=0.00003\n",
        "    EPOCH_MAX = 2\n",
        "    tf.reset_default_graph()\n",
        "    df_train, df_test = get_data100k()\n",
        "    DEVICE=\"/cpu:0\"\n",
        "\n",
        "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
        "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
        "\n",
        "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
        "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
        "    for index, row in df_train.iterrows():\n",
        "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
        "        itemid=int(row['item'])\n",
        "        AdjacencyUsers[userid][itemid]=row['rate']/5.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
        "        AdjacencyItems[itemid][userid]=row['rate']/5.0 #동일, transpose matrix에\n",
        "        DegreeUsers[userid][0]+=1\n",
        "        DegreeItems[itemid][0]+=1\n",
        "\n",
        "    DUserMax=np.amax(DegreeUsers) #max값\n",
        "    DItemMax=np.amax(DegreeItems)\n",
        "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
        "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
        "\n",
        "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
        "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
        "\n",
        "\n",
        "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
        "    print(UserFeatures.shape) #\n",
        "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    UsrDat=get_UserData100k()\n",
        "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "    ItmDat=get_ItemData100k()\n",
        "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
        "\n",
        "    UserFeaturesLength=UserFeatures.shape[1]\n",
        "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
        "\n",
        "    print(UserFeatures.shape)\n",
        "    print(ItemFeatures.shape)\n",
        "\n",
        "\n",
        "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
        "\n",
        "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"item\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
        "\n",
        "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"item\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
        "\n",
        "    #tensor 값을 할당할 placeholder 생성\n",
        "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
        "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
        "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
        "    phase = tf.placeholder(tf.bool, name='phase')\n",
        "\n",
        "    #tensor matrix생성\n",
        "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
        "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
        "\n",
        "\n",
        "    infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
        "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
        "\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "    finalerror=-1\n",
        "    train_ls = []\n",
        "    test_ls= []\n",
        "    p_ls = []\n",
        "    s_ls = []\n",
        "\n",
        "    p_dict = dict()\n",
        "    s_dict = dict()\n",
        "\n",
        "    total_df = pd.concat([df_train,df_test])\n",
        "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"item\"],total_df[\"rate\"]],batch_size=100000) #10000개?\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "        sess.run(init_op)\n",
        "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
        "        errors = deque(maxlen=samples_per_batch)\n",
        "        start = time.time()\n",
        "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
        "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
        "            users, items, rates = next(iter_train)\n",
        "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
        "                                                                  item_batch: items,\n",
        "                                                                  rate_batch: rates,\n",
        "                                                                  phase:True})\n",
        "            pred_batch = clip(pred_batch)\n",
        "            train_ls.append(pred_batch)\n",
        "            errors.append(np.power(pred_batch - rates, 2))\n",
        "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
        "                train_err = np.sqrt(np.mean(errors))\n",
        "                test_err2 = np.array([])\n",
        "                degreelist=list()\n",
        "                predlist=list()\n",
        "                for users, items, rates in iter_test: #test의 pred_batch\n",
        "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
        "                                                            item_batch: items,                                                                                             \n",
        "                                                            phase:False})\n",
        "\n",
        "                    pred_batch = clip(pred_batch)\n",
        "                    test_ls.append(pred_batch)\n",
        "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
        "                end = time.time()\n",
        "                test_err = np.sqrt(np.mean(test_err2))\n",
        "                finalerror=test_err\n",
        "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
        "                start = end\n",
        "                \n",
        "        for users, items, rates in iter_final: #test의 pred_batch\n",
        "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
        "                                                    item_batch: items,                                                                                             \n",
        "                                                    phase:False})\n",
        "        \n",
        "            p_ls.append(p_mat)\n",
        "            s_ls.append(s_mat)\n",
        "\n",
        "            concat_p = np.vstack(p_ls)\n",
        "            concat_s = np.vstack(s_ls)\n",
        "\n",
        "            user_arr = total_df.user.values\n",
        "            for idx, user in enumerate(user_arr) : \n",
        "                if user not in p_dict : \n",
        "                    p_dict[user] = concat_p[idx]\n",
        "\n",
        "            item_arr = total_df.item.values\n",
        "            for idx, item in enumerate(item_arr) : \n",
        "                if item not in s_dict : \n",
        "                    s_dict[item] = concat_s[idx]\n",
        "    \n",
        "    pred = prediction_matrix(p_dict, s_dict)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bXlN2KTu6_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "ba76f262-e189-485c-b063-8051f96bc97a"
      },
      "source": [
        "pred = GraphRec()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(943, 2626)\n",
            "(943, 2710)\n",
            "(1682, 2646)\n",
            "(?, 100)\n",
            "(?, 100)\n",
            "(?, 100)\n",
            "epoch train_error val_error elapsed_time\n",
            "  0,2.732398,2.769801,1.428543(s)\n",
            "  1,2.362379,2.085109,10.613462(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-uwerguvO_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b79cd54e-67bc-4da3-bb48-42e4333e491c"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.52850359, 2.8643779 , 2.26082502, ..., 0.39237249, 0.31340714,\n",
              "        0.27430988],\n",
              "       [3.46257497, 1.57283534, 1.27472069, ..., 0.17833491, 0.16262909,\n",
              "        0.12745946],\n",
              "       [1.99319232, 0.84380751, 0.73085707, ..., 0.10058526, 0.08469978,\n",
              "        0.07152837],\n",
              "       ...,\n",
              "       [1.61467887, 0.73686394, 0.58876031, ..., 0.08040443, 0.06205651,\n",
              "        0.06091008],\n",
              "       [3.38318705, 1.61164425, 1.29445882, ..., 0.20577695, 0.16563387,\n",
              "        0.14671638],\n",
              "       [3.47513144, 1.79410256, 1.36967216, ..., 0.21635801, 0.16596823,\n",
              "        0.16279583]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGfgoOqrtRQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
        "DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
        "\n",
        "AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
        "DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
        "for index, row in df_train.iterrows():\n",
        "    userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
        "    itemid=int(row['item'])\n",
        "    AdjacencyUsers[userid][itemid]=row['rate']/5.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
        "    AdjacencyItems[itemid][userid]=row['rate']/5.0 #동일, transpose matrix에\n",
        "    DegreeUsers[userid][0]+=1\n",
        "    DegreeItems[itemid][0]+=1\n",
        "\n",
        "DUserMax=np.amax(DegreeUsers) #max값\n",
        "DItemMax=np.amax(DegreeItems)\n",
        "DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
        "DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
        "\n",
        "AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
        "AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
        "\n",
        "\n",
        "UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
        "print(UserFeatures.shape) #\n",
        "ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "UsrDat=get_UserData100k()\n",
        "UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "ItmDat=get_ItemData100k()\n",
        "ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
        "\n",
        "UserFeaturesLength=UserFeatures.shape[1]\n",
        "ItemFeaturesLength=ItemFeatures.shape[1]\n",
        "\n",
        "print(UserFeatures.shape)\n",
        "print(ItemFeatures.shape)\n",
        "\n",
        "\n",
        "samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
        "\n",
        "iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"item\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
        "\n",
        "iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"item\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
        "\n",
        "#tensor 값을 할당할 placeholder 생성\n",
        "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
        "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
        "rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
        "phase = tf.placeholder(tf.bool, name='phase')\n",
        "\n",
        "#tensor matrix생성\n",
        "w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
        "w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
        "\n",
        "\n",
        "infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
        "global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "_, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
        "\n",
        "init_op = tf.global_variables_initializer()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "finalerror=-1\n",
        "train_ls = []\n",
        "test_ls= []\n",
        "p_ls = []\n",
        "s_ls = []\n",
        "\n",
        "p_dict = dict()\n",
        "s_dict = dict()\n",
        "\n",
        "total_df = pd.concat([df_train,df_test])\n",
        "iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"item\"],total_df[\"rate\"]],batch_size=100000) #10000개?\n",
        "\n",
        "with tf.Session(config=config) as sess:\n",
        "    sess.run(init_op)\n",
        "    print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
        "    errors = deque(maxlen=samples_per_batch)\n",
        "    start = time.time()\n",
        "    for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
        "        #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
        "        users, items, rates = next(iter_train)\n",
        "        _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
        "                                                               item_batch: items,\n",
        "                                                               rate_batch: rates,\n",
        "                                                               phase:True})\n",
        "        pred_batch = clip(pred_batch)\n",
        "        train_ls.append(pred_batch)\n",
        "        errors.append(np.power(pred_batch - rates, 2))\n",
        "        if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
        "            train_err = np.sqrt(np.mean(errors))\n",
        "            test_err2 = np.array([])\n",
        "            degreelist=list()\n",
        "            predlist=list()\n",
        "            for users, items, rates in iter_test: #test의 pred_batch\n",
        "                pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
        "                                                        item_batch: items,                                                                                             \n",
        "                                                        phase:False})\n",
        "\n",
        "                pred_batch = clip(pred_batch)\n",
        "                test_ls.append(pred_batch)\n",
        "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
        "            end = time.time()\n",
        "            test_err = np.sqrt(np.mean(test_err2))\n",
        "            finalerror=test_err\n",
        "            print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
        "            start = end\n",
        "            \n",
        "    for users, items, rates in iter_final: #test의 pred_batch\n",
        "        pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
        "                                                item_batch: items,                                                                                             \n",
        "                                                phase:False})\n",
        "    \n",
        "        p_ls.append(p_mat)\n",
        "        s_ls.append(s_mat)\n",
        "\n",
        "        concat_p = np.vstack(p_ls)\n",
        "        concat_s = np.vstack(s_ls)\n",
        "\n",
        "        user_arr = total_df.user.values\n",
        "        for idx, user in enumerate(user_arr) : \n",
        "            if user not in p_dict : \n",
        "                p_dict[user] = concat_p[idx]\n",
        "\n",
        "        item_arr = total_df.item.values\n",
        "        for idx, item in enumerate(item_arr) : \n",
        "            if item not in s_dict : \n",
        "                s_dict[item] = concat_s[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L1JHn1MyaDB",
        "colab_type": "text"
      },
      "source": [
        "# **Wearly - Graph AutoEncoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPOMpX4XuzV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37641341-e09d-4bee-ba69-cae4d9d6df4c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from six import next\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import coo_matrix\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaQhnUBBqmfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_userANDrate():\n",
        "    credentials = \"postgresql://jczdtzmaouemml:f3f55cc0c6bd25a42866864c9299f4ec79ff4ff890f6f69467e2b14dc0010074@ec2-3-215-207-12.compute-1.amazonaws.com:5432/de8i6u9p9i6vq8\"\n",
        "    dbdf = pd.read_sql(\"\"\"select distinct * from wearly_user where age <= 70 order by idx \"\"\", con = credentials)\n",
        "    item = pd.read_sql(\"\"\"select * from wearly_wear\"\"\", con = credentials)\n",
        "    df = dbdf.copy()\n",
        "    df = df.drop_duplicates(['name', 'gender', 'age']+ df.columns.tolist()[4:-1], keep='first').reset_index(drop=False)\n",
        "    df = df.drop(['index','name','idx','time'], axis=1 , inplace=False)\n",
        "    \n",
        "    print('데이터부르기 완료')\n",
        "\n",
        "    v = [data_row.values.tolist()[2:] for index, data_row in df.iterrows()]\n",
        "    vv = [v[i][j] for i in range(len(v)) for j in range(len(v[i]))]\n",
        "\n",
        "    user_rt = pd.DataFrame(index=range(0,len(df)*100) , columns=['user','image_file_name', 'rate'])\n",
        "    user_rt['user'] = sorted([i for i in range(0,len(df)) for j in range(0,100)])\n",
        "    user_rt['image_file_name'] = vv\n",
        "\n",
        "    for i in range(len(user_rt)):\n",
        "      user_rt['rate'][i] = int(user_rt['image_file_name'][i][-1])\n",
        "      user_rt['image_file_name'][i] = str(user_rt['image_file_name'][i][:-1])\n",
        "    \n",
        "    user_rt = user_rt.sample(frac=1, random_state=200).reset_index(drop=True)\n",
        "    user_rt = user_rt.merge(item[['image_id', 'image_file_name']], on='image_file_name')\n",
        "    print('user_rt 생성 완료')\n",
        "    \n",
        "    user_rt.loc[user_rt['rate'] == 3 , 'rate'] = 2\n",
        "    user_rt.loc[user_rt['rate'] == 5 , 'rate'] = 3 \n",
        "    print('숫자바꾸기 완료')\n",
        "    PERC = 0.9\n",
        "    rows = len(user_rt)\n",
        "    split_index = int(rows * PERC)\n",
        "    df_train = user_rt[0:split_index]\n",
        "    df_test = user_rt[split_index:].reset_index(drop=True)\n",
        "    print('split 완료')\n",
        "    user_mt = df[['age','gender']]\n",
        "    user_mt = user_mt.reset_index()\n",
        "    user_mt['user'] = user_mt['index']\n",
        "    user_mt = user_mt[['user', 'age', 'gender']]\n",
        "  \n",
        "    return user_mt, df_train, df_test , item, user_rt\n",
        "\n",
        "#UsrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMvJbqeVyVoB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5157e8c1-e67e-4351-eed0-8b1c1ac5cd3e"
      },
      "source": [
        "user_rt['rate'].value_counts() / user_rt['rate'].value_counts().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.471527\n",
              "1    0.288035\n",
              "3    0.240438\n",
              "Name: rate, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxJZNjj-qzVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShuffleIterator(object):\n",
        "\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        self.inputs = inputs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_cols = len(self.inputs)\n",
        "        self.len = len(self.inputs[0])\n",
        "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "\n",
        "    def next(self):\n",
        "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
        "        out = self.inputs[ids, :] #뭐임?\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "\n",
        "\n",
        "class OneEpochIterator(ShuffleIterator):\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
        "        if batch_size > 0:\n",
        "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
        "        else:\n",
        "            self.idx_group = [np.arange(self.len)]\n",
        "        self.group_id = 0\n",
        "\n",
        "    def next(self):\n",
        "        if self.group_id >= len(self.idx_group):\n",
        "            self.group_id = 0\n",
        "            raise StopIteration\n",
        "        out = self.inputs[self.idx_group[self.group_id], :]\n",
        "        self.group_id += 1\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "\n",
        "def inferenceDense(MFSIZE ,phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1, UW=0.05, IW=0.02):\n",
        "    with tf.device('/cpu:0'): \n",
        "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
        "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
        "                \n",
        "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(ul1mf.shape)\n",
        "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(il1mf.shape)\n",
        "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
        "        print(InferInputMF.shape)\n",
        "\n",
        "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
        "\n",
        "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
        "    return infer, regularizer, ul1mf, il1mf\n",
        "\n",
        "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
        "    with tf.device('/cpu:0'):\n",
        "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
        "        assert global_step is not None\n",
        "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
        "        cost = tf.add(cost_l2, regularizer)\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
        "    return cost, train_op\n",
        "\n",
        "\n",
        "def clip(x):\n",
        "    return np.clip(x, 1.0, 3.0) #벗어나는 값들 위치시키기\n",
        "\n",
        "def prediction_matrix(user_dict, item_dict):\n",
        "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
        "  for i in range(len(user_dict)):\n",
        "    for j in range(len(item_dict)):\n",
        "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
        "  return pred\n",
        "\n",
        "\n",
        "def recommender_for_user(users_items_matrix_df ,user_id, interact_matrix, df_content, topn = 6):\n",
        "    '''\n",
        "    Recommender Games for UserWarning\n",
        "    '''\n",
        "    pred_scores = interact_matrix.loc[user_id].values\n",
        "\n",
        "    df_scores   = pd.DataFrame({'image_id': list(users_items_matrix_df.columns), \n",
        "                               'score': pred_scores})\n",
        "\n",
        "    df_rec      = df_scores.set_index('image_id')\\\n",
        "                    .join(df_content.set_index('image_id'))\\\n",
        "                    .sort_values('score', ascending=False)\\\n",
        "                    .head(topn)[['score', 'image_file_name', 'hashtag_crawl']]\n",
        "    \n",
        "    return df_rec[df_rec.score > 0]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTqw8u_a1HrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GraphRec(Mf,Epoch, USERAGE, ITEMHASH, ITEMSTYLE, NORM):\n",
        "    tf.reset_default_graph()\n",
        "    usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()\n",
        "\n",
        "    USER_NUM =len(usrDat)  ; ITEM_NUM = user_rt.image_id.nunique()\n",
        "    BATCH_SIZE = 1000\n",
        "    DEVICE=\"/cpu:0\"\n",
        "    #With Graph Features\n",
        "    MFSIZE= Mf\n",
        "    UW=0.05\n",
        "    IW=0.02\n",
        "    LR=0.00003\n",
        "    EPOCH_MAX = Epoch\n",
        "\n",
        "    UsrDat = usrDat.drop(['user'],axis=1, inplace=False)\n",
        "    ItmDat = itmDat.drop(['idx', 'image_id','post_id', 'image_file_name' , 'hashtag_crawl', 'account_name'],axis=1, inplace=False)\n",
        "\n",
        "    if (USERAGE): \n",
        "      UsrDat = pd.get_dummies(UsrDat, columns=[ \"age\", \"gender\"])\n",
        "    else:\n",
        "      UsrDat = UsrDat.drop(['age'], axis=1 , inplace=False)\n",
        "      UsrDat = pd.get_dummies(UsrDat, columns=[\"gender\"])\n",
        "\n",
        "\n",
        "    if (ITEMHASH):\n",
        "      ItmDat = ItmDat\n",
        "    else:\n",
        "      ItmDat = ItmDat.drop(itmDat.columns[8:56].tolist() , axis=1 , inplace=False)\n",
        "\n",
        "\n",
        "    if (ITEMSTYLE):\n",
        "      ItmDat = ItmDat\n",
        "    else:\n",
        "      ItmDat = ItmDat.drop(itmDat.columns[56:].tolist() , axis=1 , inplace=False)\n",
        "\n",
        "    print(ItmDat.columns)\n",
        "    if (NORM):\n",
        "      ItmDat['like_num'] = np.log10(ItmDat['like_num'].values + 1)\n",
        "      ItmDat['comment_num'] = np.log10(ItmDat['comment_num'].values + 1)\n",
        "    else:\n",
        "      ItmDat = ItmDat.drop(['comment_num','like_num'],axis=1, inplace=False)\n",
        "\n",
        "\n",
        "    UsrDat = UsrDat.values\n",
        "    ItmDat = ItmDat.values\n",
        "\n",
        "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
        "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
        "\n",
        "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
        "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
        "    for index, row in df_train.iterrows():\n",
        "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
        "        itemid=int(row['image_id'])\n",
        "        AdjacencyUsers[userid][itemid]=row['rate']/3.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
        "        AdjacencyItems[itemid][userid]=row['rate']/3.0 #동일, transpose matrix에\n",
        "        DegreeUsers[userid][0]+=1\n",
        "        DegreeItems[itemid][0]+=1\n",
        "\n",
        "    DUserMax=np.amax(DegreeUsers) #max값\n",
        "    DItemMax=np.amax(DegreeItems)\n",
        "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
        "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
        "\n",
        "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
        "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
        "\n",
        "\n",
        "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
        "    print(UserFeatures.shape) \n",
        "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
        "\n",
        "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
        "\n",
        "    UserFeaturesLength=UserFeatures.shape[1]\n",
        "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
        "\n",
        "    print(UserFeatures.shape)\n",
        "    print(ItemFeatures.shape)\n",
        "\n",
        "\n",
        "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
        "\n",
        "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"image_id\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
        "\n",
        "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"image_id\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
        "\n",
        "    #tensor 값을 할당할 placeholder 생성\n",
        "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
        "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
        "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
        "    phase = tf.placeholder(tf.bool, name='phase')\n",
        "\n",
        "    #tensor matrix생성\n",
        "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
        "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
        "\n",
        "\n",
        "    infer, regularizer, p,s = inferenceDense(MFSIZE ,phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
        "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
        "\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "    finalerror=-1\n",
        "    #train_ls = []\n",
        "    #test_ls= []\n",
        "    p_ls = []\n",
        "    s_ls = []\n",
        "\n",
        "    p_dict = dict()\n",
        "    s_dict = dict()\n",
        "\n",
        "    total_df = user_rt.copy()\n",
        "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"image_id\"],total_df[\"rate\"]],batch_size=len(total_df)) #10000개?\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "        sess.run(init_op)\n",
        "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
        "        errors = deque(maxlen=samples_per_batch)\n",
        "        start = time.time()\n",
        "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
        "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
        "            users, items, rates = next(iter_train)\n",
        "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
        "                                                                  item_batch: items,\n",
        "                                                                  rate_batch: rates,\n",
        "                                                                  phase:True})\n",
        "            pred_batch = clip(pred_batch)\n",
        "            #train_ls.append(pred_batch)\n",
        "            errors.append(np.power(pred_batch - rates, 2))\n",
        "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
        "                train_err = np.sqrt(np.mean(errors))\n",
        "                test_err2 = np.array([])\n",
        "                degreelist=list()\n",
        "                predlist=list()\n",
        "                for users, items, rates in iter_test: #test의 pred_batch\n",
        "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
        "                                                            item_batch: items,                                                                                             \n",
        "                                                            phase:False})\n",
        "\n",
        "                    pred_batch = clip(pred_batch)\n",
        "                    #test_ls.append(pred_batch)\n",
        "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
        "                end = time.time()\n",
        "                test_err = np.sqrt(np.mean(test_err2))\n",
        "                finalerror=test_err\n",
        "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
        "                start = end\n",
        "\n",
        "        for users, items, rates in iter_final: #test의 pred_batch\n",
        "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
        "                                                    item_batch: items,                                                                                             \n",
        "                                                    phase:False})\n",
        "\n",
        "            p_ls.append(p_mat)\n",
        "            s_ls.append(s_mat)\n",
        "\n",
        "            concat_p = np.vstack(p_ls)\n",
        "            concat_s = np.vstack(s_ls)\n",
        "\n",
        "            user_arr = total_df.user.values\n",
        "            for idx, user in enumerate(user_arr) : \n",
        "                if user not in p_dict : \n",
        "                    p_dict[user] = concat_p[idx]\n",
        "\n",
        "            item_arr = total_df.image_id.values\n",
        "            for idx, item in enumerate(item_arr) : \n",
        "                if item not in s_dict : \n",
        "                    s_dict[item] = concat_s[idx]\n",
        "\n",
        "    pred = prediction_matrix(p_dict, s_dict)\n",
        "\n",
        "    # recommendation\n",
        "    users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
        "                                          columns = 'image_id', \n",
        "                                          values  = 'rate').fillna(0)\n",
        "\n",
        "\n",
        "    new_users_items_matrix_df  = pd.DataFrame(pred, \n",
        "                                              columns = users_items_matrix_df.columns, \n",
        "                                              index   = users_items_matrix_df.index)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
        "                                interact_matrix = new_users_items_matrix_df, \n",
        "                                df_content= itmDat)\n",
        "    \n",
        "    \n",
        "    r = users_items_matrix_df.values.astype(np.float64)\n",
        "    r[r == 0] = 'nan'\n",
        "    RMSE = np.sqrt(np.nansum((r - pred)**2 / np.isfinite(r).sum()))\n",
        "    print('Final Rmse :' , RMSE)\n",
        "    \n",
        "    return recom"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbbGTbYrnVl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "584a35c3-ec1c-4867-eb59-2d38613f0b3b"
      },
      "source": [
        "recom = GraphRec(60,20, USERAGE=True, ITEMHASH=False, ITEMSTYLE=False, NORM=True)\n",
        "recom"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터부르기 완료\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "user_rt 생성 완료\n",
            "숫자바꾸기 완료\n",
            "split 완료\n",
            "Index(['like_num', 'comment_num'], dtype='object')\n",
            "(307, 7801)\n",
            "(307, 7842)\n",
            "(7493, 7803)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b9cd0e3fcf31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphRec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUSERAGE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mITEMHASH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mITEMSTYLE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-f89b3b7113d6>\u001b[0m in \u001b[0;36mGraphRec\u001b[0;34m(Mf, Epoch, USERAGE, ITEMHASH, ITEMSTYLE, NORM)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0minfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferenceDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMFSIZE\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSER_NUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mITEM_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.09\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-878a7cd5cb23>\u001b[0m in \u001b[0;36minferenceDense\u001b[0;34m(MFSIZE, phase, user_batch, item_batch, idx_user, idx_item, user_num, item_num, UReg, IReg, UW, IW)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0muser_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"embedding_user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#idx_user에서 user_batch의 index값을 뽑음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mitem_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"embedding_item\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#w_item 이 들어감\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mul1mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMFSIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    325\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(params, ids, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(params, indices, validate_indices, name, axis)\u001b[0m\n\u001b[1;32m   2696\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m     return gen_array_ops.gather(\n\u001b[0;32m-> 2698\u001b[0;31m         params, indices, validate_indices=validate_indices, name=name)\n\u001b[0m\u001b[1;32m   2699\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(params, indices, validate_indices, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2671\u001b[0m         \u001b[0;34m\"Gather\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m         validate_indices=validate_indices, name=name)\n\u001b[0m\u001b[1;32m   2673\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3290\u001b[0m           op_def=op_def)\n\u001b[1;32m   3291\u001b[0m       self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3292\u001b[0;31m                              compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3375\u001b[0m       \u001b[0mall_colocation_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3376\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mcolocation_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_colocation_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3377\u001b[0;31m         \u001b[0mall_colocation_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocation_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocation_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolocation_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3379\u001b[0m           \u001b[0;31m# Make this device match the device of the colocated op, to provide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcolocation_groups\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     ]\n\u001b[1;32m   1750\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m       \u001b[0mclass_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m       \u001b[0;31m# This op has no explicit colocation group, so it is itself its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2238\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         raise ValueError(\n\u001b[0;32m-> 2240\u001b[0;31m             \"No attr named '\" + name + \"' in \" + str(self._node_def_val))\n\u001b[0m\u001b[1;32m   2241\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J0SKk2it6-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "1939eee2-deb7-4473-ab27-30e48b66af8d"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.57979917, 1.51902297, 1.79474604, ..., 0.36199048, 2.28392013,\n",
              "        1.28922836],\n",
              "       [1.90145825, 1.83079188, 2.18332828, ..., 0.4414312 , 2.7190154 ,\n",
              "        1.57784685],\n",
              "       [1.97139226, 1.87797406, 2.24123192, ..., 0.45384723, 2.79437954,\n",
              "        1.66069825],\n",
              "       ...,\n",
              "       [1.98293286, 1.91756914, 2.33257631, ..., 0.46988203, 2.86389438,\n",
              "        1.62340754],\n",
              "       [1.94956901, 1.84122453, 2.19870321, ..., 0.44365137, 2.71644472,\n",
              "        1.60598391],\n",
              "       [1.72659336, 1.66460662, 1.95613386, ..., 0.4036448 , 2.45776813,\n",
              "        1.44356767]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}