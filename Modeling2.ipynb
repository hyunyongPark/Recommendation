{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Modeling2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y-7DZUgoabB2",
        "W3xw5Z2fabB5",
        "fKdQbd1BabB8"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunyongPark/Recommendation/blob/master/Modeling2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTzjmT5qhdQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "9c294a85-5429-43b5-f6b8-4df2b2a441b0"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzIUc1aIhe3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "740661dd-837c-4d1a-dcd1-73b32b5c92c2"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUSnBlhBhfv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "outputId": "81180faa-8199-4ffb-d870-e9db7e2b9278"
      },
      "source": [
        "!pip install tensorflow==1.7"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/4a/42ba8d00a50a9fafc88dd5935246ecc64ffe1f6a0258ef535ffb9652140b/tensorflow-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (48.0MB)\n",
            "\u001b[K     |████████████████████████████████| 48.0MB 95kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.31.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7) (1.18.5)\n",
            "Collecting tensorboard<1.8.0,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.7) (49.6.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (3.2.2)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.8.0,>=1.7.0->tensorflow==1.7) (3.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=607462c1bb41b3fbdfdf6d4cf8a62694a528ee9669f8c2a2045c48f5e903da57\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWkPYpfCiTrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "0d70afa5-fc03-4eca-9317-c30af8b4224a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHv3nE-hBa5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cmq4lNFabBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "273b6351-188d-44db-f9ae-bb396f8327a0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from six import next\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import coo_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARU3BzHYabBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data100k():\n",
        "    global PERC\n",
        "    df = read_process(\"/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.data\", sep=\"\\t\")\n",
        "    rows = len(df)\n",
        "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
        "    split_index = int(rows * PERC)\n",
        "    df_train = df[0:split_index]\n",
        "    df_test = df[split_index:].reset_index(drop=True)\n",
        "    return df_train, df_test\n",
        "\n",
        "def get_UserData100k():\n",
        "    col_names = [\"user\", \"age\", \"gender\", \"occupation\",\"PostCode\"]\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.user', sep='|', header=None, names=col_names, engine='python')\n",
        "    del df[\"PostCode\"]\n",
        "    df[\"user\"]-=1\n",
        "    df=pd.get_dummies(df,columns=[ \"age\", \"gender\", \"occupation\"])\n",
        "    del df[\"user\"]\n",
        "    return df.values\n",
        "\n",
        "def get_ItemData100k():\n",
        "    col_names = [\"movieid\", \"movietitle\", \"releasedate\", \"videoreleasedate\",\"IMDbURL\"\n",
        "                ,\"unknown\",\"Action\",\"Adventure\",\"Animation\",\"Childrens\",\"Comedy\",\"Crime\",\"Documentary\"\n",
        "                ,\"Drama\",\"Fantasy\",\"FilmNoir\",\"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"SciFi\",\"Thriller\"\n",
        "                ,\"War\",\"Western\"]\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.item', sep='|', header=None, names=col_names, engine='python')\n",
        "    df['releasedate'] = pd.to_datetime(df['releasedate'])\n",
        "    df['year'],df['month']=zip(*df['releasedate'].map(lambda x: [x.year,x.month]))\n",
        "    df['year']-=df['year'].min()\n",
        "    df['year']/=df['year'].max()\n",
        "    df['year']=df['year'].fillna(0.0)\n",
        "\n",
        "    del df[\"month\"]\n",
        "    del df[\"movietitle\"]\n",
        "    del df[\"releasedate\"]\n",
        "    del df[\"videoreleasedate\"]\n",
        "    del df[\"IMDbURL\"]\n",
        "  \n",
        "    df[\"movieid\"]-=1\n",
        "    del  df[\"movieid\"]\n",
        "    return df.values \n",
        "\n",
        "def read_process(filname, sep=\"\\t\"):\n",
        "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
        "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
        "    df[\"user\"] -= 1\n",
        "    df[\"item\"] -= 1\n",
        "    for col in (\"user\", \"item\"):\n",
        "        df[col] = df[col].astype(np.int32)\n",
        "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_data100k():\n",
        "    global PERC\n",
        "    df = read_process(\"/content/gdrive/My Drive/recommend_practice/GraphRec/GraphRec-master/ml100k/u.data\", sep=\"\\t\")\n",
        "    rows = len(df)\n",
        "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
        "    split_index = int(rows * PERC)\n",
        "    df_train = df[0:split_index]\n",
        "    df_test = df[split_index:].reset_index(drop=True)\n",
        "    return df_train, df_test\n",
        "\n",
        "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
        "    with tf.device(DEVICE):\n",
        "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
        "        assert global_step is not None\n",
        "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
        "        cost = tf.add(cost_l2, regularizer)\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
        "    return cost, train_op\n",
        "\n",
        "def optimizationSparse(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
        "    global_step = tf.train.get_global_step()\n",
        "    assert global_step is not None\n",
        "    cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
        "    cost = tf.add(cost_l2, regularizer)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
        "    return cost, train_op\n",
        "\n",
        "def clip(x):\n",
        "    return np.clip(x, 1.0, 5.0) #벗어나는 값들 위치시키기\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc1TOwrPabB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShuffleIterator(object):\n",
        "\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        self.inputs = inputs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_cols = len(self.inputs)\n",
        "        self.len = len(self.inputs[0])\n",
        "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "\n",
        "    def next(self):\n",
        "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
        "        out = self.inputs[ids, :] #뭐임?\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "\n",
        "\n",
        "class OneEpochIterator(ShuffleIterator):\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
        "        if batch_size > 0:\n",
        "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
        "        else:\n",
        "            self.idx_group = [np.arange(self.len)]\n",
        "        self.group_id = 0\n",
        "\n",
        "    def next(self):\n",
        "        if self.group_id >= len(self.idx_group):\n",
        "            self.group_id = 0\n",
        "            raise StopIteration\n",
        "        out = self.inputs[self.idx_group[self.group_id], :]\n",
        "        self.group_id += 1\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "    \n",
        "\n",
        "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
        "    with tf.device(DEVICE): \n",
        "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\")\n",
        "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\")\n",
        "        \n",
        "        \n",
        "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
        "\n",
        "\n",
        "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\")\n",
        "\n",
        "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\")\n",
        "\n",
        "    return infer, regularizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-7DZUgoabB2",
        "colab_type": "text"
      },
      "source": [
        "## Embedding + dot.product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PxXTuyAabB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
        "    with tf.device(DEVICE): \n",
        "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
        "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
        "        \n",
        "        \n",
        "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(ul1mf.shape)\n",
        "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(il1mf.shape)\n",
        "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
        "        print(InferInputMF.shape)\n",
        "\n",
        "\n",
        "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
        "\n",
        "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
        "    return infer, regularizer, ul1mf, il1mf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s4dYfppufGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction_matrix(user_dict, item_dict):\n",
        "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
        "  for i in range(len(user_dict)):\n",
        "    for j in range(len(item_dict)):\n",
        "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C1X4jXOabB5",
        "colab_type": "text"
      },
      "source": [
        "- InferInputMF의 shape[1] 이 의미하는 것 (100) \n",
        "- mf 를 왜쓰냐\n",
        "- mf 의 output 이 어떠한 방식으로 prediction의 형태를 띄느냐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xw5Z2fabB5",
        "colab_type": "text"
      },
      "source": [
        "## GraphRec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKdQbd1BabB8",
        "colab_type": "text"
      },
      "source": [
        "## 뜯어보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9o4MLwHabCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def GraphRec():\n",
        "    BATCH_SIZE = 1000\n",
        "    USER_NUM = 943\n",
        "    ITEM_NUM = 1682\n",
        "\n",
        "    #With Graph Features\n",
        "    MFSIZE=50\n",
        "    UW=0.05\n",
        "    IW=0.02\n",
        "    LR=0.00003\n",
        "    EPOCH_MAX = 2\n",
        "    tf.reset_default_graph()\n",
        "    df_train, df_test = get_data100k()\n",
        "    DEVICE=\"/cpu:0\"\n",
        "\n",
        "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
        "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
        "\n",
        "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
        "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
        "    for index, row in df_train.iterrows():\n",
        "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
        "        itemid=int(row['item'])\n",
        "        AdjacencyUsers[userid][itemid]=row['rate']/5.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
        "        AdjacencyItems[itemid][userid]=row['rate']/5.0 #동일, transpose matrix에\n",
        "        DegreeUsers[userid][0]+=1\n",
        "        DegreeItems[itemid][0]+=1\n",
        "\n",
        "    DUserMax=np.amax(DegreeUsers) #max값\n",
        "    DItemMax=np.amax(DegreeItems)\n",
        "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
        "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
        "\n",
        "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
        "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
        "\n",
        "\n",
        "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
        "    print(UserFeatures.shape) #\n",
        "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    UsrDat=get_UserData100k()\n",
        "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "    ItmDat=get_ItemData100k()\n",
        "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
        "\n",
        "    UserFeaturesLength=UserFeatures.shape[1]\n",
        "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
        "\n",
        "    print(UserFeatures.shape)\n",
        "    print(ItemFeatures.shape)\n",
        "\n",
        "\n",
        "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
        "\n",
        "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"item\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
        "\n",
        "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"item\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
        "\n",
        "    #tensor 값을 할당할 placeholder 생성\n",
        "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
        "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
        "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
        "    phase = tf.placeholder(tf.bool, name='phase')\n",
        "\n",
        "    #tensor matrix생성\n",
        "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
        "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
        "\n",
        "\n",
        "    infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
        "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
        "\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "    finalerror=-1\n",
        "    train_ls = []\n",
        "    test_ls= []\n",
        "    p_ls = []\n",
        "    s_ls = []\n",
        "\n",
        "    p_dict = dict()\n",
        "    s_dict = dict()\n",
        "\n",
        "    total_df = pd.concat([df_train,df_test])\n",
        "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"item\"],total_df[\"rate\"]],batch_size=100000) #10000개?\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "        sess.run(init_op)\n",
        "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
        "        errors = deque(maxlen=samples_per_batch)\n",
        "        start = time.time()\n",
        "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
        "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
        "            users, items, rates = next(iter_train)\n",
        "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
        "                                                                  item_batch: items,\n",
        "                                                                  rate_batch: rates,\n",
        "                                                                  phase:True})\n",
        "            pred_batch = clip(pred_batch)\n",
        "            train_ls.append(pred_batch)\n",
        "            errors.append(np.power(pred_batch - rates, 2))\n",
        "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
        "                train_err = np.sqrt(np.mean(errors))\n",
        "                test_err2 = np.array([])\n",
        "                degreelist=list()\n",
        "                predlist=list()\n",
        "                for users, items, rates in iter_test: #test의 pred_batch\n",
        "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
        "                                                            item_batch: items,                                                                                             \n",
        "                                                            phase:False})\n",
        "\n",
        "                    pred_batch = clip(pred_batch)\n",
        "                    test_ls.append(pred_batch)\n",
        "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
        "                end = time.time()\n",
        "                test_err = np.sqrt(np.mean(test_err2))\n",
        "                finalerror=test_err\n",
        "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
        "                start = end\n",
        "                \n",
        "        for users, items, rates in iter_final: #test의 pred_batch\n",
        "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
        "                                                    item_batch: items,                                                                                             \n",
        "                                                    phase:False})\n",
        "        \n",
        "            p_ls.append(p_mat)\n",
        "            s_ls.append(s_mat)\n",
        "\n",
        "            concat_p = np.vstack(p_ls)\n",
        "            concat_s = np.vstack(s_ls)\n",
        "\n",
        "            user_arr = total_df.user.values\n",
        "            for idx, user in enumerate(user_arr) : \n",
        "                if user not in p_dict : \n",
        "                    p_dict[user] = concat_p[idx]\n",
        "\n",
        "            item_arr = total_df.item.values\n",
        "            for idx, item in enumerate(item_arr) : \n",
        "                if item not in s_dict : \n",
        "                    s_dict[item] = concat_s[idx]\n",
        "    \n",
        "    pred = prediction_matrix(p_dict, s_dict)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bXlN2KTu6_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "ba76f262-e189-485c-b063-8051f96bc97a"
      },
      "source": [
        "pred = GraphRec()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(943, 2626)\n",
            "(943, 2710)\n",
            "(1682, 2646)\n",
            "(?, 100)\n",
            "(?, 100)\n",
            "(?, 100)\n",
            "epoch train_error val_error elapsed_time\n",
            "  0,2.732398,2.769801,1.428543(s)\n",
            "  1,2.362379,2.085109,10.613462(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-uwerguvO_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b79cd54e-67bc-4da3-bb48-42e4333e491c"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.52850359, 2.8643779 , 2.26082502, ..., 0.39237249, 0.31340714,\n",
              "        0.27430988],\n",
              "       [3.46257497, 1.57283534, 1.27472069, ..., 0.17833491, 0.16262909,\n",
              "        0.12745946],\n",
              "       [1.99319232, 0.84380751, 0.73085707, ..., 0.10058526, 0.08469978,\n",
              "        0.07152837],\n",
              "       ...,\n",
              "       [1.61467887, 0.73686394, 0.58876031, ..., 0.08040443, 0.06205651,\n",
              "        0.06091008],\n",
              "       [3.38318705, 1.61164425, 1.29445882, ..., 0.20577695, 0.16563387,\n",
              "        0.14671638],\n",
              "       [3.47513144, 1.79410256, 1.36967216, ..., 0.21635801, 0.16596823,\n",
              "        0.16279583]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGfgoOqrtRQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
        "DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
        "\n",
        "AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
        "DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
        "for index, row in df_train.iterrows():\n",
        "    userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
        "    itemid=int(row['item'])\n",
        "    AdjacencyUsers[userid][itemid]=row['rate']/5.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
        "    AdjacencyItems[itemid][userid]=row['rate']/5.0 #동일, transpose matrix에\n",
        "    DegreeUsers[userid][0]+=1\n",
        "    DegreeItems[itemid][0]+=1\n",
        "\n",
        "DUserMax=np.amax(DegreeUsers) #max값\n",
        "DItemMax=np.amax(DegreeItems)\n",
        "DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
        "DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
        "\n",
        "AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
        "AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
        "\n",
        "\n",
        "UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
        "print(UserFeatures.shape) #\n",
        "ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "UsrDat=get_UserData100k()\n",
        "UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "ItmDat=get_ItemData100k()\n",
        "ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
        "\n",
        "UserFeaturesLength=UserFeatures.shape[1]\n",
        "ItemFeaturesLength=ItemFeatures.shape[1]\n",
        "\n",
        "print(UserFeatures.shape)\n",
        "print(ItemFeatures.shape)\n",
        "\n",
        "\n",
        "samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
        "\n",
        "iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"item\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
        "\n",
        "iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"item\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
        "\n",
        "#tensor 값을 할당할 placeholder 생성\n",
        "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
        "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
        "rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
        "phase = tf.placeholder(tf.bool, name='phase')\n",
        "\n",
        "#tensor matrix생성\n",
        "w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
        "w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
        "\n",
        "\n",
        "infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
        "global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "_, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
        "\n",
        "init_op = tf.global_variables_initializer()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "finalerror=-1\n",
        "train_ls = []\n",
        "test_ls= []\n",
        "p_ls = []\n",
        "s_ls = []\n",
        "\n",
        "p_dict = dict()\n",
        "s_dict = dict()\n",
        "\n",
        "total_df = pd.concat([df_train,df_test])\n",
        "iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"item\"],total_df[\"rate\"]],batch_size=100000) #10000개?\n",
        "\n",
        "with tf.Session(config=config) as sess:\n",
        "    sess.run(init_op)\n",
        "    print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
        "    errors = deque(maxlen=samples_per_batch)\n",
        "    start = time.time()\n",
        "    for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
        "        #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
        "        users, items, rates = next(iter_train)\n",
        "        _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
        "                                                               item_batch: items,\n",
        "                                                               rate_batch: rates,\n",
        "                                                               phase:True})\n",
        "        pred_batch = clip(pred_batch)\n",
        "        train_ls.append(pred_batch)\n",
        "        errors.append(np.power(pred_batch - rates, 2))\n",
        "        if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
        "            train_err = np.sqrt(np.mean(errors))\n",
        "            test_err2 = np.array([])\n",
        "            degreelist=list()\n",
        "            predlist=list()\n",
        "            for users, items, rates in iter_test: #test의 pred_batch\n",
        "                pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
        "                                                        item_batch: items,                                                                                             \n",
        "                                                        phase:False})\n",
        "\n",
        "                pred_batch = clip(pred_batch)\n",
        "                test_ls.append(pred_batch)\n",
        "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
        "            end = time.time()\n",
        "            test_err = np.sqrt(np.mean(test_err2))\n",
        "            finalerror=test_err\n",
        "            print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
        "            start = end\n",
        "            \n",
        "    for users, items, rates in iter_final: #test의 pred_batch\n",
        "        pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
        "                                                item_batch: items,                                                                                             \n",
        "                                                phase:False})\n",
        "    \n",
        "        p_ls.append(p_mat)\n",
        "        s_ls.append(s_mat)\n",
        "\n",
        "        concat_p = np.vstack(p_ls)\n",
        "        concat_s = np.vstack(s_ls)\n",
        "\n",
        "        user_arr = total_df.user.values\n",
        "        for idx, user in enumerate(user_arr) : \n",
        "            if user not in p_dict : \n",
        "                p_dict[user] = concat_p[idx]\n",
        "\n",
        "        item_arr = total_df.item.values\n",
        "        for idx, item in enumerate(item_arr) : \n",
        "            if item not in s_dict : \n",
        "                s_dict[item] = concat_s[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L1JHn1MyaDB",
        "colab_type": "text"
      },
      "source": [
        "# **Wearly - Graph AutoEncoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPOMpX4XuzV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8080713c-435b-4801-95a3-d804b06fc1ef"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from six import next\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import coo_matrix\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaQhnUBBqmfo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "4c5916a8-bfcb-4ef1-9307-2107a37f33b7"
      },
      "source": [
        "def make_userANDrate():\n",
        "    credentials = \"postgresql://jczdtzmaouemml:f3f55cc0c6bd25a42866864c9299f4ec79ff4ff890f6f69467e2b14dc0010074@ec2-3-215-207-12.compute-1.amazonaws.com:5432/de8i6u9p9i6vq8\"\n",
        "    dbdf = pd.read_sql(\"\"\"select distinct * from wearly_user where age <= 70 order by idx \"\"\", con = credentials)\n",
        "    item = pd.read_sql(\"\"\"select * from wearly_wear\"\"\", con = credentials)\n",
        "    df = dbdf.copy()\n",
        "    df = df.drop_duplicates(['name', 'gender', 'age']+ df.columns.tolist()[4:-1], keep='first').reset_index(drop=False)\n",
        "    df = df.drop(['index','name','idx','time'], axis=1 , inplace=False)\n",
        "    \n",
        "    print('데이터부르기 완료')\n",
        "\n",
        "    v = [data_row.values.tolist()[2:] for index, data_row in df.iterrows()]\n",
        "    vv = [v[i][j] for i in range(len(v)) for j in range(len(v[i]))]\n",
        "\n",
        "    user_rt = pd.DataFrame(index=range(0,len(df)*100) , columns=['user','image_file_name', 'rate'])\n",
        "    user_rt['user'] = sorted([i for i in range(0,len(df)) for j in range(0,100)])\n",
        "    user_rt['image_file_name'] = vv\n",
        "\n",
        "    for i in range(len(user_rt)):\n",
        "      user_rt['rate'][i] = int(user_rt['image_file_name'][i][-1])\n",
        "      user_rt['image_file_name'][i] = str(user_rt['image_file_name'][i][:-1])\n",
        "    \n",
        "    user_rt = user_rt.sample(frac=1, random_state=200).reset_index(drop=True)\n",
        "    user_rt = user_rt.merge(item[['image_id', 'image_file_name']], on='image_file_name')\n",
        "    print('user_rt 생성 완료')\n",
        "    \n",
        "    user_rt.loc[user_rt['rate'] == 3 , 'rate'] = 2\n",
        "    user_rt.loc[user_rt['rate'] == 5 , 'rate'] = 3 \n",
        "    print('숫자바꾸기 완료')\n",
        "    PERC = 0.9\n",
        "    rows = len(user_rt)\n",
        "    split_index = int(rows * PERC)\n",
        "    df_train = user_rt[0:split_index]\n",
        "    df_test = user_rt[split_index:].reset_index(drop=True)\n",
        "    print('split 완료')\n",
        "    user_mt = df[['age','gender']]\n",
        "    user_mt = user_mt.reset_index()\n",
        "    user_mt['user'] = user_mt['index']\n",
        "    user_mt = user_mt[['user', 'age', 'gender']]\n",
        "    \n",
        "    # get one-hot encoding (gender)\n",
        "    user_mt = pd.get_dummies(user_mt, columns=[ \"age\", \"gender\"])\n",
        "  \n",
        "    return user_mt, df_train, df_test , item, user_rt\n",
        "\n",
        "UsrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터부르기 완료\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "user_rt 생성 완료\n",
            "숫자바꾸기 완료\n",
            "split 완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3OnvwKAX3Vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "aab0afde-cb31-4a7c-b892-9e97516a4d49"
      },
      "source": [
        "itmDat"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>image_id</th>\n",
              "      <th>post_id</th>\n",
              "      <th>image_file_name</th>\n",
              "      <th>hashtag_crawl</th>\n",
              "      <th>like_num</th>\n",
              "      <th>comment_num</th>\n",
              "      <th>account_name</th>\n",
              "      <th>fashion</th>\n",
              "      <th>ootd</th>\n",
              "      <th>fashionblogger</th>\n",
              "      <th>instafashion</th>\n",
              "      <th>fashionista</th>\n",
              "      <th>streetstyle</th>\n",
              "      <th>outfit</th>\n",
              "      <th>instagood</th>\n",
              "      <th>fashionable</th>\n",
              "      <th>fashionstyle</th>\n",
              "      <th>stylish</th>\n",
              "      <th>outfitoftheday</th>\n",
              "      <th>styleblogger</th>\n",
              "      <th>moda</th>\n",
              "      <th>love</th>\n",
              "      <th>model</th>\n",
              "      <th>look</th>\n",
              "      <th>streetwear</th>\n",
              "      <th>photooftheday</th>\n",
              "      <th>streetfashion</th>\n",
              "      <th>instastyle</th>\n",
              "      <th>fashionweek</th>\n",
              "      <th>photography</th>\n",
              "      <th>trend</th>\n",
              "      <th>fashiongram</th>\n",
              "      <th>beautiful</th>\n",
              "      <th>fashionblog</th>\n",
              "      <th>fashionaddict</th>\n",
              "      <th>beauty</th>\n",
              "      <th>summer</th>\n",
              "      <th>fashiondiaries</th>\n",
              "      <th>fashionpost</th>\n",
              "      <th>fashioninspiration</th>\n",
              "      <th>lookoftheday</th>\n",
              "      <th>dress</th>\n",
              "      <th>blogger</th>\n",
              "      <th>picoftheday</th>\n",
              "      <th>lookbook</th>\n",
              "      <th>girl</th>\n",
              "      <th>mensfashion</th>\n",
              "      <th>cute</th>\n",
              "      <th>follow</th>\n",
              "      <th>instagram</th>\n",
              "      <th>fashionshow</th>\n",
              "      <th>lifestyle</th>\n",
              "      <th>shopping</th>\n",
              "      <th>fashioninsta</th>\n",
              "      <th>dailylook</th>\n",
              "      <th>sporty</th>\n",
              "      <th>casual</th>\n",
              "      <th>modern</th>\n",
              "      <th>elegant</th>\n",
              "      <th>natural</th>\n",
              "      <th>glamorous</th>\n",
              "      <th>sophisticated</th>\n",
              "      <th>grunge</th>\n",
              "      <th>retro</th>\n",
              "      <th>romantic</th>\n",
              "      <th>sexy</th>\n",
              "      <th>military</th>\n",
              "      <th>ethnic</th>\n",
              "      <th>classic</th>\n",
              "      <th>business_casual</th>\n",
              "      <th>manish</th>\n",
              "      <th>exotic</th>\n",
              "      <th>goth_punk_rocker</th>\n",
              "      <th>hiphop</th>\n",
              "      <th>hippie</th>\n",
              "      <th>tomboy</th>\n",
              "      <th>preppy</th>\n",
              "      <th>kitsch_kidult</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>B0-12mlBGcn</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/d...</td>\n",
              "      <td>dailyfashion</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>B002Ungh9yO</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/d...</td>\n",
              "      <td>dailyfashion</td>\n",
              "      <td>156</td>\n",
              "      <td>7</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>B00a282hKx8</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/d...</td>\n",
              "      <td>dailyfashion</td>\n",
              "      <td>2869</td>\n",
              "      <td>62</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>B00DtFbls4i</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/d...</td>\n",
              "      <td>dailyfashion</td>\n",
              "      <td>77</td>\n",
              "      <td>6</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>B00Mc7pFFnO</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/d...</td>\n",
              "      <td>dailyfashion</td>\n",
              "      <td>74</td>\n",
              "      <td>5</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7488</th>\n",
              "      <td>7488</td>\n",
              "      <td>7488</td>\n",
              "      <td>B1ZcqgFnLu4</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/t...</td>\n",
              "      <td>trend</td>\n",
              "      <td>0</td>\n",
              "      <td>99</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7489</th>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>B1ZpQCpDU1p</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/t...</td>\n",
              "      <td>trend</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7490</th>\n",
              "      <td>7490</td>\n",
              "      <td>7490</td>\n",
              "      <td>By3n_5aHSLu</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/t...</td>\n",
              "      <td>trend</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7491</th>\n",
              "      <td>7491</td>\n",
              "      <td>7491</td>\n",
              "      <td>Bz34ivXFwFs</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/t...</td>\n",
              "      <td>trend</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7492</th>\n",
              "      <td>7492</td>\n",
              "      <td>7492</td>\n",
              "      <td>Bz9eUzVl7hv</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/t...</td>\n",
              "      <td>trend</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7493 rows × 79 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       idx  image_id      post_id  ... tomboy preppy  kitsch_kidult\n",
              "0        0         0  B0-12mlBGcn  ...      0      0              0\n",
              "1        1         1  B002Ungh9yO  ...      0      0              0\n",
              "2        2         2  B00a282hKx8  ...      0      0              0\n",
              "3        3         3  B00DtFbls4i  ...      0      0              0\n",
              "4        4         4  B00Mc7pFFnO  ...      0      0              0\n",
              "...    ...       ...          ...  ...    ...    ...            ...\n",
              "7488  7488      7488  B1ZcqgFnLu4  ...      0      0              0\n",
              "7489  7489      7489  B1ZpQCpDU1p  ...      0      0              0\n",
              "7490  7490      7490  By3n_5aHSLu  ...      0      0              0\n",
              "7491  7491      7491  Bz34ivXFwFs  ...      0      0              0\n",
              "7492  7492      7492  Bz9eUzVl7hv  ...      0      0              0\n",
              "\n",
              "[7493 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxJZNjj-qzVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShuffleIterator(object):\n",
        "\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        self.inputs = inputs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_cols = len(self.inputs)\n",
        "        self.len = len(self.inputs[0])\n",
        "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "\n",
        "    def next(self):\n",
        "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
        "        out = self.inputs[ids, :] #뭐임?\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "\n",
        "\n",
        "class OneEpochIterator(ShuffleIterator):\n",
        "    def __init__(self, inputs, batch_size=10):\n",
        "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
        "        if batch_size > 0:\n",
        "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
        "        else:\n",
        "            self.idx_group = [np.arange(self.len)]\n",
        "        self.group_id = 0\n",
        "\n",
        "    def next(self):\n",
        "        if self.group_id >= len(self.idx_group):\n",
        "            self.group_id = 0\n",
        "            raise StopIteration\n",
        "        out = self.inputs[self.idx_group[self.group_id], :]\n",
        "        self.group_id += 1\n",
        "        return [out[:, i] for i in range(self.num_cols)]\n",
        "\n",
        "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1, MFSIZE=50, UW=0.05, IW=0.02):\n",
        "    with tf.device('/cpu:0'): \n",
        "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
        "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
        "                \n",
        "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(ul1mf.shape)\n",
        "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        print(il1mf.shape)\n",
        "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
        "        print(InferInputMF.shape)\n",
        "\n",
        "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
        "\n",
        "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
        "    return infer, regularizer, ul1mf, il1mf\n",
        "\n",
        "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
        "    with tf.device('/cpu:0'):\n",
        "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
        "        assert global_step is not None\n",
        "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
        "        cost = tf.add(cost_l2, regularizer)\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
        "    return cost, train_op\n",
        "\n",
        "\n",
        "def clip(x):\n",
        "    return np.clip(x, 1.0, 3.0) #벗어나는 값들 위치시키기\n",
        "\n",
        "def prediction_matrix(user_dict, item_dict):\n",
        "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
        "  for i in range(len(user_dict)):\n",
        "    for j in range(len(item_dict)):\n",
        "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
        "  return pred\n",
        "\n",
        "\n",
        "def recommender_for_user(users_items_matrix_df ,user_id, interact_matrix, df_content, topn = 6):\n",
        "    '''\n",
        "    Recommender Games for UserWarning\n",
        "    '''\n",
        "    pred_scores = interact_matrix.loc[user_id].values\n",
        "\n",
        "    df_scores   = pd.DataFrame({'image_id': list(users_items_matrix_df.columns), \n",
        "                               'score': pred_scores})\n",
        "\n",
        "    df_rec      = df_scores.set_index('image_id')\\\n",
        "                    .join(df_content.set_index('image_id'))\\\n",
        "                    .sort_values('score', ascending=False)\\\n",
        "                    .head(topn)[['score', 'image_file_name', 'hashtag_crawl']]\n",
        "    \n",
        "    return df_rec[df_rec.score > 0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTqw8u_a1HrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GraphRec():\n",
        "    tf.reset_default_graph()\n",
        "    usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()\n",
        "\n",
        "    USER_NUM =len(usrDat)  ; ITEM_NUM = user_rt.image_id.nunique()\n",
        "    BATCH_SIZE = 1000\n",
        "    DEVICE=\"/cpu:0\"\n",
        "    #With Graph Features\n",
        "    MFSIZE=50\n",
        "    UW=0.05\n",
        "    IW=0.02\n",
        "    LR=0.00003\n",
        "    EPOCH_MAX = 2\n",
        "\n",
        "    UsrDat = usrDat.drop(['user'],axis=1, inplace=False)\n",
        "    ItmDat = itmDat.drop(['idx', 'image_id','post_id', 'image_file_name' , 'hashtag_crawl', 'account_name','comment_num','like_num'],axis=1, inplace=False)\n",
        "\n",
        "    UsrDat = UsrDat.values\n",
        "    ItmDat = ItmDat.values\n",
        "\n",
        "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
        "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
        "\n",
        "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
        "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
        "    for index, row in df_train.iterrows():\n",
        "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
        "        itemid=int(row['image_id'])\n",
        "        AdjacencyUsers[userid][itemid]=row['rate']/3.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
        "        AdjacencyItems[itemid][userid]=row['rate']/3.0 #동일, transpose matrix에\n",
        "        DegreeUsers[userid][0]+=1\n",
        "        DegreeItems[itemid][0]+=1\n",
        "\n",
        "    DUserMax=np.amax(DegreeUsers) #max값\n",
        "    DItemMax=np.amax(DegreeItems)\n",
        "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
        "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
        "\n",
        "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
        "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
        "\n",
        "\n",
        "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
        "    print(UserFeatures.shape) #\n",
        "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
        "\n",
        "\n",
        "\n",
        "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
        "\n",
        "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
        "\n",
        "    UserFeaturesLength=UserFeatures.shape[1]\n",
        "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
        "\n",
        "    print(UserFeatures.shape)\n",
        "    print(ItemFeatures.shape)\n",
        "\n",
        "\n",
        "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
        "\n",
        "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"image_id\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
        "\n",
        "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"image_id\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
        "\n",
        "    #tensor 값을 할당할 placeholder 생성\n",
        "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
        "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
        "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
        "    phase = tf.placeholder(tf.bool, name='phase')\n",
        "\n",
        "    #tensor matrix생성\n",
        "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
        "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
        "\n",
        "\n",
        "    infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
        "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
        "\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "    finalerror=-1\n",
        "    #train_ls = []\n",
        "    #test_ls= []\n",
        "    p_ls = []\n",
        "    s_ls = []\n",
        "\n",
        "    p_dict = dict()\n",
        "    s_dict = dict()\n",
        "\n",
        "    total_df = user_rt.copy()\n",
        "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"image_id\"],total_df[\"rate\"]],batch_size=len(total_df)) #10000개?\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "        sess.run(init_op)\n",
        "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
        "        errors = deque(maxlen=samples_per_batch)\n",
        "        start = time.time()\n",
        "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
        "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
        "            users, items, rates = next(iter_train)\n",
        "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
        "                                                                  item_batch: items,\n",
        "                                                                  rate_batch: rates,\n",
        "                                                                  phase:True})\n",
        "            pred_batch = clip(pred_batch)\n",
        "            #train_ls.append(pred_batch)\n",
        "            errors.append(np.power(pred_batch - rates, 2))\n",
        "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
        "                train_err = np.sqrt(np.mean(errors))\n",
        "                test_err2 = np.array([])\n",
        "                degreelist=list()\n",
        "                predlist=list()\n",
        "                for users, items, rates in iter_test: #test의 pred_batch\n",
        "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
        "                                                            item_batch: items,                                                                                             \n",
        "                                                            phase:False})\n",
        "\n",
        "                    pred_batch = clip(pred_batch)\n",
        "                    #test_ls.append(pred_batch)\n",
        "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
        "                end = time.time()\n",
        "                test_err = np.sqrt(np.mean(test_err2))\n",
        "                finalerror=test_err\n",
        "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
        "                start = end\n",
        "                \n",
        "        for users, items, rates in iter_final: #test의 pred_batch\n",
        "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
        "                                                    item_batch: items,                                                                                             \n",
        "                                                    phase:False})\n",
        "        \n",
        "            p_ls.append(p_mat)\n",
        "            s_ls.append(s_mat)\n",
        "\n",
        "            concat_p = np.vstack(p_ls)\n",
        "            concat_s = np.vstack(s_ls)\n",
        "\n",
        "            user_arr = total_df.user.values\n",
        "            for idx, user in enumerate(user_arr) : \n",
        "                if user not in p_dict : \n",
        "                    p_dict[user] = concat_p[idx]\n",
        "\n",
        "            item_arr = total_df.image_id.values\n",
        "            for idx, item in enumerate(item_arr) : \n",
        "                if item not in s_dict : \n",
        "                    s_dict[item] = concat_s[idx]\n",
        "\n",
        "    pred = prediction_matrix(p_dict, s_dict)\n",
        "\n",
        "    # recommendation\n",
        "    users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
        "                                          columns = 'image_id', \n",
        "                                          values  = 'rate').fillna(0)\n",
        "\n",
        "\n",
        "    new_users_items_matrix_df  = pd.DataFrame(pred, \n",
        "                                              columns = users_items_matrix_df.columns, \n",
        "                                              index   = users_items_matrix_df.index)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
        "                                interact_matrix = new_users_items_matrix_df, \n",
        "                                df_content= itmDat)\n",
        "    \n",
        "    return pred , recom"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbbGTbYrnVl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "9ce940f2-7839-4d88-b26a-0bd4ddd720b8"
      },
      "source": [
        "pred, recom = GraphRec()\n",
        "recom"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터부르기 완료\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "user_rt 생성 완료\n",
            "숫자바꾸기 완료\n",
            "split 완료\n",
            "(307, 7801)\n",
            "(307, 7842)\n",
            "(7493, 7872)\n",
            "(?, 100)\n",
            "(?, 100)\n",
            "(?, 100)\n",
            "epoch train_error val_error elapsed_time\n",
            "  0,1.204990,1.196186,2.650040(s)\n",
            "  1,1.198131,1.196186,6.403559(s)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>image_file_name</th>\n",
              "      <th>hashtag_crawl</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3599</th>\n",
              "      <td>0.400103</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/i...</td>\n",
              "      <td>instafashion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>0.388498</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/s...</td>\n",
              "      <td>stylish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>0.387274</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/f...</td>\n",
              "      <td>fashionblogger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>0.378451</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/f...</td>\n",
              "      <td>fashionable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7491</th>\n",
              "      <td>0.377920</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/t...</td>\n",
              "      <td>trend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3315</th>\n",
              "      <td>0.370719</td>\n",
              "      <td>https://wearlyimages.s3.amazonaws.com/wearly/f...</td>\n",
              "      <td>fashion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             score  ...   hashtag_crawl\n",
              "image_id            ...                \n",
              "3599      0.400103  ...    instafashion\n",
              "6853      0.388498  ...         stylish\n",
              "1063      0.387274  ...  fashionblogger\n",
              "751       0.378451  ...     fashionable\n",
              "7491      0.377920  ...           trend\n",
              "3315      0.370719  ...         fashion\n",
              "\n",
              "[6 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J0SKk2it6-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "14cf52d2-978d-4f47-96e0-d7d277eb24fd"
      },
      "source": [
        "pred"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.11506895, 0.10305187, 0.14185498, ..., 0.0391383 , 0.28916515,\n",
              "        0.11259902],\n",
              "       [0.15334623, 0.12890827, 0.17957723, ..., 0.04835087, 0.39423509,\n",
              "        0.12497647],\n",
              "       [0.09877338, 0.11424694, 0.1378563 , ..., 0.03903639, 0.29577817,\n",
              "        0.12247111],\n",
              "       ...,\n",
              "       [0.14472929, 0.11533771, 0.15145712, ..., 0.05010597, 0.338046  ,\n",
              "        0.11561311],\n",
              "       [0.17827914, 0.11090808, 0.20319847, ..., 0.06201185, 0.38052126,\n",
              "        0.12317877],\n",
              "       [0.12910883, 0.11004409, 0.15541034, ..., 0.03682112, 0.37792043,\n",
              "        0.11662647]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAB3o3WTVWJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}